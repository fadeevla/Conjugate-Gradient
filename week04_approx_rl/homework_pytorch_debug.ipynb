{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can get 1 pt (instead of 3 pts) for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioIEVODJZdC9",
        "outputId": "ff0d1f3f-f105-40b7-a14f-c75dfce8647d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    os.makedirs('dqn', exist_ok=True)\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/atari_wrappers.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/utils.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/replay_buffer.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/framebuffer.py -P dqn/\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/dqn/analysis.py -P dqn/\n",
        "\n",
        "    !pip install gymnasium\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import dqn.utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AmFXRrkqZdC-",
        "outputId": "57f2554b-fbdf-4f58-95b5-aa7b522ccd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape, n_actions"
      ],
      "metadata": {
        "id": "erEZ4PQPZtwc",
        "outputId": "73574b6d-5fce-43e9-846e-ae784b805cd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4,), 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UVlpkvZOZdC-",
        "outputId": "1bf7032d-5392-44d8-e4c3-30d079f542b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        assert len(state_shape) == 1\n",
        "        state_dim = state_shape[0]\n",
        "\n",
        "        # Define a simple feedforward neural network\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),  # Input layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),  # Hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)  # Output layer for Q-values\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = self.network(state_t)\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert (\n",
        "            len(qvalues.shape) == 2 and\n",
        "            qvalues.shape[0] == state_t.shape[0] and\n",
        "            qvalues.shape[1] == n_actions\n",
        "        )\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bv1s5JKzZdC-"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "e-Sg1cqPZdC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wQEHwR1AZdC_"
      },
      "outputs": [],
      "source": [
        "from dqn.replay_buffer import ReplayBuffer\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0RnFX5sfZdC_"
      },
      "outputs": [],
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends, add record with done=True and reset the game.\n",
        "    It is guaranteed that env has done=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    print(s)\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # ---------------------START OF YOUR CODE---------------------#\n",
        "    done = False\n",
        "    for _ in range(n_steps):\n",
        "      qvalues = agent.get_qvalues(s)\n",
        "      print(qvalues)\n",
        "      action = agent.sample_actions(qvalues)[0]\n",
        "      next_s, r, terminated, truncated, _ = env.step(action)\n",
        "      if terminated or truncated:\n",
        "        done = True\n",
        "\n",
        "      exp_replay.add(s, action, r, next_s, done)\n",
        "      sum_rewards +=r\n",
        "      s = next_s\n",
        "      if done:\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "    # ---------------------END OF YOUR CODE---------------------#\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ZXXmFEKGZdC_",
        "outputId": "88575251-2b31-472e-b3e5-67f06bcc2c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.02126075 -0.04409892  0.01296786 -0.0120857 ]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7dbf3c459839>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplay_and_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# if you're using your own experience replay buffer, some of those tests may need correction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d9bb1f0e21a3>\u001b[0m in \u001b[0;36mplay_and_record\u001b[0;34m(initial_state, agent, env, exp_replay, n_steps)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fe565056f37a>\u001b[0m in \u001b[0;36mget_qvalues\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fe565056f37a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state_t)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstate_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \\\n",
        "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
        "    \"but instead added %i\" % len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, \\\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
        "        np.mean(is_dones), len(exp_replay))\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \\\n",
        "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \\\n",
        "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (10,), \\\n",
        "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
        "        \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BLJCNiuZdC_"
      },
      "outputs": [],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxrEOC7mZdC_"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \\\n",
        "    \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \\\n",
        "    \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \\\n",
        "    \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOk81bdZZdC_"
      },
      "outputs": [],
      "source": [
        "seed = <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13K5t2CTZdDA"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD7PAlwQZdDA"
      },
      "outputs": [],
      "source": [
        "from dqn.utils import is_enough_ram, linear_decay\n",
        "\n",
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not is_enough_ram(min_available_gb=0.1):\n",
        "        print(\"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "             )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        <YOUR CODE: sample batch_size of data from experience replay>\n",
        "\n",
        "        loss = <YOUR CODE: compute TD loss>\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm)\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            <YOUR CODE>\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history\")\n",
        "            plt.plot(td_loss_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history\")\n",
        "            plt.plot(grad_norm_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(\n",
        "  make_env(),\n",
        "  agent, n_games=30, greedy=True, t_max=1000\n",
        ")\n",
        "print('final score:', final_score)\n",
        "assert final_score > 300, 'not good enough for DQN'\n",
        "print('Well done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "from analysis import play_and_log_episode\n",
        "\n",
        "eval_env = make_env()\n",
        "record = play_and_log_episode(eval_env, agent)\n",
        "print('total reward for life:', np.sum(record['rewards']))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record['v_mc'], record['v_agent'])\n",
        "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
        "       'black', linestyle='--', label='x=y')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title('State Value Estimates')\n",
        "ax.set_xlabel('Monte-Carlo')\n",
        "ax.set_ylabel('Agent')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}